{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NAVER 한달살기 - 클러스터링(1. 위계적)\n",
    "\n",
    " 참조사이트\n",
    " -  https://m.blog.naver.com/j7youngh/222864205826"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install matplotlib seaborn sklearn konlpy scikit-learn wordcloud pyLDAvis gensim\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import re\n",
    "\n",
    "from konlpy.tag import Okt\n",
    "okt = Okt() \n",
    "from collections import Counter\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.rc('font', family='malgun Gothic') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>contents</th>\n",
       "      <th>time</th>\n",
       "      <th>blog_text</th>\n",
       "      <th>key</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>돌아기랑 제주도한달살기 24일째(406일) : 비양도</td>\n",
       "      <td>첨에 제주도 한달살기 올때도 어디갈지 생각을 안하고 와서 매번 전날이나 아침에 오늘...</td>\n",
       "      <td>2023.11.27</td>\n",
       "      <td>첨에 제주도 한달살기 올때도 어디갈지 생각을 안하고 와서 매번 전날이나 아침에 오늘...</td>\n",
       "      <td>한달살기</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>제주도 한달살기 (20) - 우도 코코나라, 문빵구, 돌담집, 몬딱...</td>\n",
       "      <td>한달살기 중 두번째 우도 ㅎㅎ 처음은 전기차를 빌렸지만 이번엔 카트형식으로 나란히 ...</td>\n",
       "      <td>2023.11.07</td>\n",
       "      <td>한달살기 중 두번째 우도 ㅎㅎ처음은 전기차를 빌렸지만 이번엔 카트형식으로 나란히 앉...</td>\n",
       "      <td>한달살기</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>제주동쪽숙소 제주도한달살기하기 좋은 구좌 제주오션뷰2차♪</td>\n",
       "      <td>#제주오션뷰2차 #제주도펜션 #제주동쪽숙소 제주에는 크게 동쪽과 서쪽으로 나뉘면서 ...</td>\n",
       "      <td>2023.12.13</td>\n",
       "      <td>#제주오션뷰2차#제주도펜션#제주동쪽숙소제주에는 크게 동쪽과 서쪽으로 나뉘면서 다양한...</td>\n",
       "      <td>한달살기</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>제주도 한달살기 비용 (한달살이 생활비)</td>\n",
       "      <td>제주도 한달살기는 돈이 적게 들지는 않지만 우리 가족에게 꽤 많은 추억을 남겼으므로...</td>\n",
       "      <td>2022.02.23</td>\n",
       "      <td>2021년 4월에 제주도 한달살기를 하고 왔다.세식구 기준(어른 둘, 아기 하나)으...</td>\n",
       "      <td>한달살기</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>제주도 한달살기 숙소 비용 제주 중문 애견동반 더맨션 이이공공</td>\n",
       "      <td>그중에서도 제일 마음에 들었던 건 오후 시간 깊숙이 들어오는 가을 햇살 창밖에 보이...</td>\n",
       "      <td>2023.10.14</td>\n",
       "      <td>이번에는 좀 여유로운 여행을 하고 싶어서다소 긴 일정으로 제주도에 방문했어요.여행...</td>\n",
       "      <td>한달살기</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1932</th>\n",
       "      <td>제주도한달살이</td>\n",
       "      <td>드뎌 육지다~ 뒤로 보이는 육지가 제주항이네요. 3다도인 제주에 좋은 추억도 많이 ...</td>\n",
       "      <td>2020.12.18</td>\n",
       "      <td>#제주도 #제주한달살이#실버크클라우드코로나19로 발이묶인 윈드서퍼들의차선택은~~ 한...</td>\n",
       "      <td>한달살이</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1933</th>\n",
       "      <td>[제주도 한달살이] 제주도 게스트하우스 스텝 후기...</td>\n",
       "      <td>연돈 예약 성공해서 연돈도 먹고, 한라산 등산도 가고, 스쿠버다이빙도 배우고, 저녁...</td>\n",
       "      <td>2023.05.28</td>\n",
       "      <td>22년12월28일~23년2월4일약 한달동안 제주살이를 했다.(글 마지막에 스텝 꿀팁...</td>\n",
       "      <td>한달살이</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1934</th>\n",
       "      <td>제주도 한달살이 시작!!!</td>\n",
       "      <td>무계획으로 출발 하는 제주도 한달살이!!! 퇴사를 하고 나니 새로운 환경에서 살아보...</td>\n",
       "      <td>2022.03.16</td>\n",
       "      <td>무계획으로 출발 하는 제주도 한달살이!!!퇴사를 하고 나니 새로운 환경에서 살아보고...</td>\n",
       "      <td>한달살이</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1935</th>\n",
       "      <td>[제주도 한달살이 이야기] 평산제면소에서 마제소바먹고 러닝후...</td>\n",
       "      <td>제주도에선 술 잘 안먹겠다고 다짐했는데 거의 하루 한캔은 아니여도 한모금씩은 매일 ...</td>\n",
       "      <td>2022.02.20</td>\n",
       "      <td>오늘은 숙소 근처에 있다는 마제소바 집에 들렸어요맛집이라고 해서 저번에 왔었는데그때...</td>\n",
       "      <td>한달살이</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1936</th>\n",
       "      <td>제주도 한달살이 day 1</td>\n",
       "      <td>친구가 제주도 한달살이 하러 간다고해서 백수몬인 나도 쫄래쫄래 쫓아가서 2주 있으려...</td>\n",
       "      <td>2022.06.11</td>\n",
       "      <td>친구가 제주도 한달살이 하러 간다고해서백수몬인 나도 쫄래쫄래 쫓아가서2주 있으려다 ...</td>\n",
       "      <td>한달살이</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1937 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         title  \\\n",
       "0                돌아기랑 제주도한달살기 24일째(406일) : 비양도   \n",
       "1     제주도 한달살기 (20) - 우도 코코나라, 문빵구, 돌담집, 몬딱...   \n",
       "2              제주동쪽숙소 제주도한달살기하기 좋은 구좌 제주오션뷰2차♪   \n",
       "3                       제주도 한달살기 비용 (한달살이 생활비)   \n",
       "4           제주도 한달살기 숙소 비용 제주 중문 애견동반 더맨션 이이공공   \n",
       "...                                        ...   \n",
       "1932                                   제주도한달살이   \n",
       "1933            [제주도 한달살이] 제주도 게스트하우스 스텝 후기...   \n",
       "1934                            제주도 한달살이 시작!!!   \n",
       "1935      [제주도 한달살이 이야기] 평산제면소에서 마제소바먹고 러닝후...   \n",
       "1936                            제주도 한달살이 day 1   \n",
       "\n",
       "                                               contents        time  \\\n",
       "0     첨에 제주도 한달살기 올때도 어디갈지 생각을 안하고 와서 매번 전날이나 아침에 오늘...  2023.11.27   \n",
       "1     한달살기 중 두번째 우도 ㅎㅎ 처음은 전기차를 빌렸지만 이번엔 카트형식으로 나란히 ...  2023.11.07   \n",
       "2     #제주오션뷰2차 #제주도펜션 #제주동쪽숙소 제주에는 크게 동쪽과 서쪽으로 나뉘면서 ...  2023.12.13   \n",
       "3     제주도 한달살기는 돈이 적게 들지는 않지만 우리 가족에게 꽤 많은 추억을 남겼으므로...  2022.02.23   \n",
       "4     그중에서도 제일 마음에 들었던 건 오후 시간 깊숙이 들어오는 가을 햇살 창밖에 보이...  2023.10.14   \n",
       "...                                                 ...         ...   \n",
       "1932  드뎌 육지다~ 뒤로 보이는 육지가 제주항이네요. 3다도인 제주에 좋은 추억도 많이 ...  2020.12.18   \n",
       "1933  연돈 예약 성공해서 연돈도 먹고, 한라산 등산도 가고, 스쿠버다이빙도 배우고, 저녁...  2023.05.28   \n",
       "1934  무계획으로 출발 하는 제주도 한달살이!!! 퇴사를 하고 나니 새로운 환경에서 살아보...  2022.03.16   \n",
       "1935  제주도에선 술 잘 안먹겠다고 다짐했는데 거의 하루 한캔은 아니여도 한모금씩은 매일 ...  2022.02.20   \n",
       "1936  친구가 제주도 한달살이 하러 간다고해서 백수몬인 나도 쫄래쫄래 쫓아가서 2주 있으려...  2022.06.11   \n",
       "\n",
       "                                              blog_text   key  \n",
       "0     첨에 제주도 한달살기 올때도 어디갈지 생각을 안하고 와서 매번 전날이나 아침에 오늘...  한달살기  \n",
       "1     한달살기 중 두번째 우도 ㅎㅎ처음은 전기차를 빌렸지만 이번엔 카트형식으로 나란히 앉...  한달살기  \n",
       "2     #제주오션뷰2차#제주도펜션#제주동쪽숙소제주에는 크게 동쪽과 서쪽으로 나뉘면서 다양한...  한달살기  \n",
       "3     2021년 4월에 제주도 한달살기를 하고 왔다.세식구 기준(어른 둘, 아기 하나)으...  한달살기  \n",
       "4      이번에는 좀 여유로운 여행을 하고 싶어서다소 긴 일정으로 제주도에 방문했어요.여행...  한달살기  \n",
       "...                                                 ...   ...  \n",
       "1932  #제주도 #제주한달살이#실버크클라우드코로나19로 발이묶인 윈드서퍼들의차선택은~~ 한...  한달살이  \n",
       "1933  22년12월28일~23년2월4일약 한달동안 제주살이를 했다.(글 마지막에 스텝 꿀팁...  한달살이  \n",
       "1934  무계획으로 출발 하는 제주도 한달살이!!!퇴사를 하고 나니 새로운 환경에서 살아보고...  한달살이  \n",
       "1935  오늘은 숙소 근처에 있다는 마제소바 집에 들렸어요맛집이라고 해서 저번에 왔었는데그때...  한달살이  \n",
       "1936  친구가 제주도 한달살이 하러 간다고해서백수몬인 나도 쫄래쫄래 쫓아가서2주 있으려다 ...  한달살이  \n",
       "\n",
       "[1937 rows x 5 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('./data/naver_month.csv')\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "title        0\n",
       "contents     0\n",
       "time         0\n",
       "blog_text    0\n",
       "key          0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# df 수집 오류 결과 확인\n",
    "(df == 'error').sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "title        0\n",
       "contents     0\n",
       "time         0\n",
       "blog_text    3\n",
       "key          0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isnull().sum()\n",
    "# blog_text가 nan인 행 존재 -> 제외"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>contents</th>\n",
       "      <th>time</th>\n",
       "      <th>blog_text</th>\n",
       "      <th>key</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1807</th>\n",
       "      <td>제주도 한달살이 스물한번째 날</td>\n",
       "      <td>제주도 한달살이 스물한번째 날입니다 오늘은 제주도에 오후부터 비가 예보 되있어서 나...</td>\n",
       "      <td>2022.03.30</td>\n",
       "      <td>NaN</td>\n",
       "      <td>한달살이</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1882</th>\n",
       "      <td>제주도 한달살이 스물여덟번째 날</td>\n",
       "      <td>제주도 한달살이 스물여덟번째 날입니다 이제 제주도 살이 삼일 남았습니다 이번주는 날...</td>\n",
       "      <td>2022.04.07</td>\n",
       "      <td>NaN</td>\n",
       "      <td>한달살이</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1919</th>\n",
       "      <td>제주도 한달살이 열다섯번째 날</td>\n",
       "      <td>제주도 한달살이 열다섯번째 날 입니다 벌써 한달중 절반이 지나가는군요 아침에 일어나...</td>\n",
       "      <td>2022.03.24</td>\n",
       "      <td>NaN</td>\n",
       "      <td>한달살이</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  title                                           contents  \\\n",
       "1807   제주도 한달살이 스물한번째 날  제주도 한달살이 스물한번째 날입니다 오늘은 제주도에 오후부터 비가 예보 되있어서 나...   \n",
       "1882  제주도 한달살이 스물여덟번째 날  제주도 한달살이 스물여덟번째 날입니다 이제 제주도 살이 삼일 남았습니다 이번주는 날...   \n",
       "1919   제주도 한달살이 열다섯번째 날  제주도 한달살이 열다섯번째 날 입니다 벌써 한달중 절반이 지나가는군요 아침에 일어나...   \n",
       "\n",
       "            time blog_text   key  \n",
       "1807  2022.03.30       NaN  한달살이  \n",
       "1882  2022.04.07       NaN  한달살이  \n",
       "1919  2022.03.24       NaN  한달살이  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[df['blog_text'].isna() == 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# blog_text가 nan인 겨우 제외\n",
    "df = df[df['blog_text'].isna() != 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 클러스터링 1 - 위계적\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "594\n",
      "739\n"
     ]
    }
   ],
   "source": [
    "stopwords = set([])\n",
    "\n",
    "web_stopwords = pd.read_csv(\"https://raw.githubusercontent.com/yoonkt200/FastCampusDataset/master/korean_stopwords.txt\").values.tolist()\n",
    "web_stopwords[:10]\n",
    "\n",
    "for i in web_stopwords:\n",
    "    stopwords.add(i[0])\n",
    "\n",
    "print(len(stopwords))\n",
    "\n",
    "jeju_stopwords = [ '리뷰', '숙소', '정말', '동안', '다음', '바로', '근처', '조금', '시간',\n",
    "                   '호스트', '정도', '방문', '사장', '이용', '다시', '예약', '살이', '살기', '아주', '추천', '하루', '이번',\n",
    "                   '처음', '달동', '저희', '하나', '사용', '모두', '진짜', '일주일', '보름', '선택', '시작', '한번', '가장', '마음',\n",
    "                   '제외', '이동', '무엇', '준비', '그냥', '감사',  '덕분', '후기', '기회',  '다른', '여기', '우리',\n",
    "                   '미스터', '미스터멘션', '멘션', '때문', '오늘', '도착', '보고', '마지막', '가지', '이제', '지금', '모습', '위해',\n",
    "                   '타고', '사실', '포스팅', '주문', '역시', '침대', '거실', '매일', '출발', '소개', '기억', '참고', '계속', '객실',\n",
    "                   '일차', '이야기', '요즘', '이유', '경우', '일단', '거의', '스텝', '제일', '자주', '어디', '마무리', '아래', '운영',\n",
    "                   '이상', '의자', '별로', '원래', '중간', '뭔가', '첫째', '체크', '잠시', '언제', '살짝', '해도', '계단', '부분', '냉장고',\n",
    "                   '침실', '기본', '제공', '일찍', '인테리어', '완전', '추가', '등등', '매우', '가야', '그때', '기간', '마치', '가득', '대부분',\n",
    "                   '미리', '둘째', '근무', '공간', '스테이', '알파', '엄마', '아빠', '아들', '일정', '계획', '아침', '주방', '서울', '육지',\n",
    "                   '점심', '저녁', '자리', '두리', '언니', '가격', '건물', '숙박', '만원', '가기', '코로나' ,                   \n",
    "                   '호텔', '펜션', '게스트하우스', '독채','화장실', '욕실', '숙박', '세탁기', '가면', '직접', '비용', '길이',\n",
    "                    '여행', '트립', '제주', '제주도', '생각', '느낌',\n",
    "                    '광고', '스텝', '블로그', '개월', '도서관', '스탭'\n",
    "                   ]\n",
    "for word in jeju_stopwords:\n",
    "    stopwords.add(word)\n",
    "\n",
    "print(len(stopwords))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 한글만 추출해서 okt로 명사화하고\n",
    "# 명사의 길이가 2이상이고, 불용어에 해당하지 않는 것들만 모으는\n",
    "def text_cleaning(text):\n",
    "    hangul = re.compile('[^ ㄱ-ㅣ 가-힣]')  # 정규 표현식 처리\n",
    "    result = hangul.sub('', text)\n",
    "    okt = Okt()  # 형태소 추출\n",
    "    nouns = okt.nouns(result)\n",
    "    nouns = [x for x in nouns if len(x) > 1]  # 한글자 키워드 제거\n",
    "    nouns = [x for x in nouns if x not in stopwords]  # 불용어 제거\n",
    "    return nouns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>CountVectorizer(tokenizer=&lt;function &lt;lambda&gt; at 0x000001973197D8B0&gt;)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">CountVectorizer</label><div class=\"sk-toggleable__content\"><pre>CountVectorizer(tokenizer=&lt;function &lt;lambda&gt; at 0x000001973197D8B0&gt;)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "CountVectorizer(tokenizer=<function <lambda> at 0x000001973197D8B0>)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# CountVectorizer - 단어를 tokenize 해서 \n",
    "vect = CountVectorizer(tokenizer = lambda x: text_cleaning(x))\n",
    "vect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\song\\venvs\\project1\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:525: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[11], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m bow_vect \u001b[38;5;241m=\u001b[39m \u001b[43mvect\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mblog_text\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtolist\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      2\u001b[0m bow_vect\n",
      "File \u001b[1;32md:\\song\\venvs\\project1\\lib\\site-packages\\sklearn\\base.py:1152\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1145\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[0;32m   1147\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m   1148\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m   1149\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1150\u001b[0m     )\n\u001b[0;32m   1151\u001b[0m ):\n\u001b[1;32m-> 1152\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fit_method(estimator, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32md:\\song\\venvs\\project1\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:1389\u001b[0m, in \u001b[0;36mCountVectorizer.fit_transform\u001b[1;34m(self, raw_documents, y)\u001b[0m\n\u001b[0;32m   1381\u001b[0m             warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[0;32m   1382\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUpper case characters found in\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1383\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m vocabulary while \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlowercase\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1384\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m is True. These entries will not\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1385\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m be matched with any documents\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1386\u001b[0m             )\n\u001b[0;32m   1387\u001b[0m             \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m-> 1389\u001b[0m vocabulary, X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_count_vocab\u001b[49m\u001b[43m(\u001b[49m\u001b[43mraw_documents\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfixed_vocabulary_\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1391\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbinary:\n\u001b[0;32m   1392\u001b[0m     X\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mfill(\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[1;32md:\\song\\venvs\\project1\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:1276\u001b[0m, in \u001b[0;36mCountVectorizer._count_vocab\u001b[1;34m(self, raw_documents, fixed_vocab)\u001b[0m\n\u001b[0;32m   1274\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m doc \u001b[38;5;129;01min\u001b[39;00m raw_documents:\n\u001b[0;32m   1275\u001b[0m     feature_counter \u001b[38;5;241m=\u001b[39m {}\n\u001b[1;32m-> 1276\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m feature \u001b[38;5;129;01min\u001b[39;00m \u001b[43manalyze\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdoc\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[0;32m   1277\u001b[0m         \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1278\u001b[0m             feature_idx \u001b[38;5;241m=\u001b[39m vocabulary[feature]\n",
      "File \u001b[1;32md:\\song\\venvs\\project1\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:112\u001b[0m, in \u001b[0;36m_analyze\u001b[1;34m(doc, analyzer, tokenizer, ngrams, preprocessor, decoder, stop_words)\u001b[0m\n\u001b[0;32m    110\u001b[0m     doc \u001b[38;5;241m=\u001b[39m preprocessor(doc)\n\u001b[0;32m    111\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m tokenizer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 112\u001b[0m     doc \u001b[38;5;241m=\u001b[39m \u001b[43mtokenizer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdoc\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    113\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ngrams \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    114\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m stop_words \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "Cell \u001b[1;32mIn[9], line 4\u001b[0m, in \u001b[0;36m<lambda>\u001b[1;34m(x)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfeature_extraction\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtext\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m CountVectorizer\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# CountVectorizer - 단어를 tokenize 해서 \u001b[39;00m\n\u001b[1;32m----> 4\u001b[0m vect \u001b[38;5;241m=\u001b[39m CountVectorizer(tokenizer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mlambda\u001b[39;00m x: \u001b[43mtext_cleaning\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m      5\u001b[0m vect\n",
      "Cell \u001b[1;32mIn[8], line 7\u001b[0m, in \u001b[0;36mtext_cleaning\u001b[1;34m(text)\u001b[0m\n\u001b[0;32m      5\u001b[0m result \u001b[38;5;241m=\u001b[39m hangul\u001b[38;5;241m.\u001b[39msub(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m, text)\n\u001b[0;32m      6\u001b[0m okt \u001b[38;5;241m=\u001b[39m Okt()  \u001b[38;5;66;03m# 형태소 추출\u001b[39;00m\n\u001b[1;32m----> 7\u001b[0m nouns \u001b[38;5;241m=\u001b[39m \u001b[43mokt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnouns\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresult\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      8\u001b[0m nouns \u001b[38;5;241m=\u001b[39m [x \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m nouns \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(x) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m]  \u001b[38;5;66;03m# 한글자 키워드 제거\u001b[39;00m\n\u001b[0;32m      9\u001b[0m nouns \u001b[38;5;241m=\u001b[39m [x \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m nouns \u001b[38;5;28;01mif\u001b[39;00m x \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m stopwords]  \u001b[38;5;66;03m# 불용어 제거\u001b[39;00m\n",
      "File \u001b[1;32md:\\song\\venvs\\project1\\lib\\site-packages\\konlpy\\tag\\_okt.py:83\u001b[0m, in \u001b[0;36mOkt.nouns\u001b[1;34m(self, phrase)\u001b[0m\n\u001b[0;32m     80\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mnouns\u001b[39m(\u001b[38;5;28mself\u001b[39m, phrase):\n\u001b[0;32m     81\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Noun extractor.\"\"\"\u001b[39;00m\n\u001b[1;32m---> 83\u001b[0m     tagged \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpos\u001b[49m\u001b[43m(\u001b[49m\u001b[43mphrase\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     84\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [s \u001b[38;5;28;01mfor\u001b[39;00m s, t \u001b[38;5;129;01min\u001b[39;00m tagged \u001b[38;5;28;01mif\u001b[39;00m t \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mNoun\u001b[39m\u001b[38;5;124m'\u001b[39m]\n",
      "File \u001b[1;32md:\\song\\venvs\\project1\\lib\\site-packages\\konlpy\\tag\\_okt.py:71\u001b[0m, in \u001b[0;36mOkt.pos\u001b[1;34m(self, phrase, norm, stem, join)\u001b[0m\n\u001b[0;32m     59\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"POS tagger.\u001b[39;00m\n\u001b[0;32m     60\u001b[0m \u001b[38;5;124;03mIn contrast to other classes in this subpackage,\u001b[39;00m\n\u001b[0;32m     61\u001b[0m \u001b[38;5;124;03mthis POS tagger doesn't have a `flatten` option,\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     67\u001b[0m \u001b[38;5;124;03m:param join: If True, returns joined sets of morph and tag.\u001b[39;00m\n\u001b[0;32m     68\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     69\u001b[0m validate_phrase_inputs(phrase)\n\u001b[1;32m---> 71\u001b[0m tokens \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjki\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtokenize\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     72\u001b[0m \u001b[43m            \u001b[49m\u001b[43mphrase\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     73\u001b[0m \u001b[43m            \u001b[49m\u001b[43mjpype\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjava\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlang\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mBoolean\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnorm\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     74\u001b[0m \u001b[43m            \u001b[49m\u001b[43mjpype\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjava\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlang\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mBoolean\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstem\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mtoArray()\n\u001b[0;32m     75\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m join:\n\u001b[0;32m     76\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [t \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m tokens]\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "bow_vect = vect.fit_transform(df['blog_text'].tolist())\n",
    "bow_vect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'bow_vect' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mbow_vect\u001b[49m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'bow_vect' is not defined"
     ]
    }
   ],
   "source": [
    "word_list = vect.get_feature_names_out()\n",
    "word_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_list = bow_vect.toarray().sum(axis=0)\n",
    "count_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = []\n",
    "for k, v in sorted(vect.vocabulary_.items(), key=lambda item:item[1]): \n",
    "  columns.append(k)\n",
    "\n",
    "df = pd.DataFrame(bow_vect.toarray(), columns = columns) # DTM 만들기\n",
    "df_tdm = df.T   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "594\n",
      "739\n"
     ]
    }
   ],
   "source": [
    "df_tdm['total'] = df_tdm.sum(axis=1)     # 단어 총 빈도수 계산\n",
    "df_words = df_tdm[df_tdm['total'].rank(ascending=False) <= 50] # 빈도수 상위 50개 추출\n",
    "df_words= df_words.drop('total', axis=1)    # 컬럼 'total' 제거\n",
    "df_words[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.cluster.hierarchy import linkage, dendrogram\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 군집 방식은 ward로 거리는 유크리디안으로 측정해 군집분석\n",
    "\n",
    "clusters = linkage(df_words, method='ward', metric='euclidean')\n",
    "\n",
    "# 덴드로그램 그리기\n",
    "\n",
    "plt.figure(figsize=(20, 10))               # 이미지 크기 설정\n",
    "dendrogram(clusters,\n",
    "           leaf_rotation=50,               # 라벨 50% 기울리기\n",
    "           leaf_font_size=10,              # 라벨 폰트 크기\n",
    "           labels=df_words.index)          # 라벨에 사용할 변수\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<gensim.interfaces.TransformedCorpus at 0x1bd68601d00>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.cluster import AgglomerativeClustering\n",
    "\n",
    "ward = AgglomerativeClustering(n_clusters=5, affinity='euclidean', linkage='ward')\n",
    "predict = ward.fit_predict(df_words)      # 군집 3개 분리값(0, 1, 2)\n",
    "df_words['predict'] = predict             # 해당 군집값(0, 1, 2)을 원본 데이터에 붙이기\n",
    "df_words[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0,\n",
      "  '0.015*\"오름\" + 0.015*\"코스\" + 0.012*\"바다\" + 0.011*\"올레길\" + 0.008*\"해수욕장\" + '\n",
      "  '0.008*\"사람\" + 0.008*\"해변\" + 0.007*\"풍경\" + 0.006*\"숲길\" + 0.005*\"한라산\"'),\n",
      " (1,\n",
      "  '0.009*\"바다\" + 0.007*\"위치\" + 0.007*\"하우스\" + 0.006*\"가족\" + 0.006*\"서귀포\" + '\n",
      "  '0.005*\"거리\" + 0.004*\"시설\" + 0.004*\"마당\" + 0.003*\"카페\" + 0.003*\"애월\"'),\n",
      " (2,\n",
      "  '0.018*\"맛집\" + 0.014*\"아기\" + 0.013*\"카페\" + 0.009*\"식당\" + 0.008*\"돼지\" + 0.007*\"메뉴\" '\n",
      "  '+ 0.006*\"고기\" + 0.006*\"바다\" + 0.005*\"애월\" + 0.005*\"음식\"'),\n",
      " (3,\n",
      "  '0.019*\"카페\" + 0.014*\"사람\" + 0.007*\"바다\" + 0.007*\"친구\" + 0.005*\"날씨\" + 0.004*\"커피\" '\n",
      "  '+ 0.004*\"비행기\" + 0.004*\"구경\" + 0.004*\"공항\" + 0.004*\"맛집\"'),\n",
      " (4,\n",
      "  '0.012*\"체험\" + 0.008*\"바다\" + 0.007*\"우도\" + 0.006*\"사람\" + 0.005*\"벚꽃\" + 0.005*\"날씨\" '\n",
      "  '+ 0.005*\"폭포\" + 0.004*\"가족\" + 0.004*\"요금\" + 0.004*\"물놀이\"')]\n"
     ]
    }
   ],
   "source": [
    "cluster_1 = df_words[df_words['predict'] == 0].index\n",
    "print(\"군집 1: \", cluster_1)\n",
    "cluster_2 = df_words[df_words['predict'] == 1].index\n",
    "print(\"군집 2: \", cluster_2)\n",
    "cluster_3 = df_words[df_words['predict'] == 2].index\n",
    "print(\"군집 3: \", cluster_3)\n",
    "cluster_4 = df_words[df_words['predict'] == 3].index\n",
    "print(\"군집 4: \", cluster_4)\n",
    "cluster_5 = df_words[df_words['predict'] == 4].index\n",
    "print(\"군집 5: \", cluster_5)\n",
    "# cluster_6 = df_words[df_words['predict'] == 5].index\n",
    "# print(\"군집 6: \", cluster_6)\n",
    "# cluster_7 = df_words[df_words['predict'] == 6].index\n",
    "# print(\"군집 7: \", cluster_7)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "project1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
